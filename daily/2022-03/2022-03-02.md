# 2022-03-02

## transformers bert

今天复现 transformers 的 bert ner 模型。踩了两个坑：

1. 多个短句子拼接起来做 NER，不能乱选拼接符，比如之前用分号，导致准确率一直是 0，其实 bert tokenizer 有自带的一系列 `[unusedX]` ，预留给下游任务使用。
2. 一条完整的语料，前面必须加上 `[CLS]` ，因为预训练的时候一般都是这么训练出来的，使用的时候也要用这种模式，AB 句之间使用 `[SEP]` 拼接，如果没有 B 句，那也要在最后加一个 `[SEP]`, 否则它可能感知不到句子的结束位置。

## transformers dataset offline 模式

由于模型需要放到内网来跑。transformers 案例都是从互联网上临时加载数据集，有两种方法可以把数据集下载下来离线使用。

## 拷贝 cache 目录

外网运行一次数据加载，数据会自动加载到 `~/.cache` 目录, 把它拷贝到内网就好了。
这个 cache 目录也可以通过 `XDG_CACHE_HOME` 环境变量配置的。
拷贝到内网后，还需要设置 `HF_DATASETS_OFFLINE=true` 启用离线模式。

## 手动保存数据

外网下载并导出数据到某个目录

```python
import datasets

data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
```

内网加载数据目录

```python
import datasets

data = datasets.load_from_disk(/SAVED/DATA/DIR)
```
